import streamlit as st
import torch
import unicodedata
from transformers import AutoModelForCausalLM, AutoTokenizer

# ==== Streamlit UI ====
st.set_page_config(
    page_title="L·ª•c B√°t Generator",
    page_icon="üå∏",
    layout="wide"
)

left_col, right_col = st.columns([1, 2], gap="large")

with left_col:
    st.image("truyen-kieu.jpg", use_container_width=True)

with right_col:
    st.title("L·ª•c B√°t Generator")
    st.markdown("""
    This app generates a **b√°t line (8 syllables)** that completes your **l·ª•c line (6 syllables)** input using a fine-tuned GPT-2 model.<br>
    Model: <a href="https://huggingface.co/melanieyes/melanie-poem-generation" target="_blank">melanieyes/melanie-poem-generation</a>
    """, unsafe_allow_html=True)

    with st.expander("üìú Instructions", expanded=True):
        st.markdown("""
        1. Enter a valid 6-syllable *l·ª•c* line.  
        2. Click **Generate** to get a matching *b√°t* line.
        """)

    col1, col2 = st.columns([3, 1])
    luc_line = col1.text_input("‚úçÔ∏è L·ª•c Line (6 syllables):", "trƒÉng v√†ng in b√≥ng b√™n th·ªÅm")
    generate_clicked = col2.button("üìå Generate")

# ==== Load model/tokenizer ====
@st.cache_resource
def load_model_and_tokenizer():
    model = AutoModelForCausalLM.from_pretrained("melanieyes/melanie-poem-generation")
    tokenizer = AutoTokenizer.from_pretrained("melanieyes/melanie-poem-generation", use_fast=False)
    tokenizer.pad_token = tokenizer.eos_token
    model.to("cuda" if torch.cuda.is_available() else "cpu")
    model.eval()
    return model, tokenizer

model, tokenizer = load_model_and_tokenizer()

# ==== L·ª•c B√°t tone rule ====
def get_tone_class(syllable):
    syllable = unicodedata.normalize('NFC', syllable.lower())
    for char in syllable[::-1]:
        if char in '√†√®√¨√≤√π·ª≥' or char in 'aeiouy':
            return 'b·∫±ng'
        elif char in '√°√©√≠√≥√∫√Ω·∫£·∫ª·ªâ·ªè·ªß·ª∑√£·∫Ωƒ©√µ≈©·ªπ·∫°·∫π·ªã·ªç·ª•·ªµ':
            return 'tr·∫Øc'
    return 'b·∫±ng'

def check_luc_bat_rule(line6, line8):
    w6 = line6.strip().split()
    w8 = line8.strip().split()
    if len(w6) != 6 or len(w8) != 8:
        return False

    tone6 = [get_tone_class(w) for w in w6]
    tone8 = [get_tone_class(w) for w in w8]

    return (
        tone6[2] == 'b·∫±ng' and tone6[4] == 'tr·∫Øc' and tone6[5] == 'b·∫±ng' and
        tone8[2] == 'b·∫±ng' and tone8[4] == 'tr·∫Øc' and tone8[5] == 'b·∫±ng' and tone8[6] == 'tr·∫Øc' and tone8[7] == 'b·∫±ng'
    )

# ==== Generation ====
def generate_bat_line(model, tokenizer, luc_line, max_attempts=10):
    for _ in range(max_attempts):
        inputs = tokenizer(luc_line, return_tensors="pt").to(model.device)
        output = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=40,
            temperature=0.9,
            top_k=40,
            top_p=0.95,
            repetition_penalty=1.1,
            no_repeat_ngram_size=2,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id
        )
        decoded = tokenizer.decode(output[0], skip_special_tokens=True)
        words = decoded.strip().split()
        for i in range(len(words) - 8 + 1):
            candidate = " ".join(words[i:i+8])
            if check_luc_bat_rule(luc_line, candidate):
                return candidate
    return "[FAILED]"

# ==== Output ====
if generate_clicked:
    with st.spinner("‚ú® Generating..."):
        bat_line = generate_bat_line(model, tokenizer, luc_line)
        st.subheader("üå∏ L·ª•c B√°t Couple:")
        st.text(luc_line.strip())
        st.text(bat_line.strip())
